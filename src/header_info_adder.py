"""
é¡µå¤´ä¿¡æ¯æ·»åŠ æ¨¡å—
åœ¨ç¿»è¯‘åçš„HTMLæ–‡ä»¶é¡µå¤´æ·»åŠ åŸæ–‡é“¾æ¥å’Œç¿»è¯‘è€…ä¿¡æ¯

åˆ›å»ºæ—¶é—´ï¼š2024-12-19
é¡¹ç›®ï¼šç³»åˆ—æŠ€æœ¯æ–‡ç« ç¿»è¯‘
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from urllib.parse import urlparse
from bs4 import BeautifulSoup, Tag

from config.settings import TRANS_DIR


class HeaderInfoAdder:
    """åœ¨ç¿»è¯‘åçš„ç½‘é¡µå¤´éƒ¨æ·»åŠ åŸæ–‡é“¾æ¥å’Œç¿»è¯‘è€…ä¿¡æ¯"""

    def __init__(self, trans_dir: str = None, urls_config: str = None):
        """
        åˆå§‹åŒ–é¡µå¤´ä¿¡æ¯æ·»åŠ å™¨

        Args:
            trans_dir (str): ç¿»è¯‘åæ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„TRANS_DIR
            urls_config (str): URLé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨config/urls.txt
        """
        self.logger = logging.getLogger(__name__)

        # è®¾ç½®ç›®å½•è·¯å¾„
        self.trans_dir = Path(trans_dir) if trans_dir else Path(TRANS_DIR)
        self.urls_config = urls_config or "src/config/urls.txt"

        # éªŒè¯ç›®å½•å­˜åœ¨
        if not self.trans_dir.exists():
            raise FileNotFoundError(f"ç¿»è¯‘ç›®å½•ä¸å­˜åœ¨: {self.trans_dir}")

        # æ–‡ä»¶ååˆ°åŸå§‹URLçš„æ˜ å°„å­—å…¸
        self.file_url_mapping: Dict[str, str] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'files_processed': 0,
            'files_modified': 0,
            'headers_added': 0,
            'files_skipped': 0
        }

        # ç¿»è¯‘è€…ä¿¡æ¯
        self.translator_name = "åŒ—æçš„æ ‘"
        self.wechat_qr_url = "https://wechat-account-1251781786.cos.ap-guangzhou.myqcloud.com/wechat_account.jpeg"

        self.logger.info(f"é¡µå¤´ä¿¡æ¯æ·»åŠ å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡ç›®å½•: {self.trans_dir}")

    def build_file_url_mapping(self) -> Dict[str, str]:
        """
        æ„å»ºæ–‡ä»¶ååˆ°åŸå§‹URLçš„æ˜ å°„å…³ç³»

        Returns:
            Dict[str, str]: æ–‡ä»¶åæ˜ å°„å­—å…¸
        """
        try:
            self.file_url_mapping.clear()

            # è·å–å®é™…å­˜åœ¨çš„HTMLæ–‡ä»¶
            existing_files = set()
            for html_file in self.trans_dir.glob("*.html"):
                existing_files.add(html_file.name)

            self.logger.info(f"æ‰¾åˆ° {len(existing_files)} ä¸ªç°æœ‰HTMLæ–‡ä»¶")

            # è¯»å–URLé…ç½®æ–‡ä»¶
            urls_file = Path(self.urls_config)
            if urls_file.exists():
                with open(urls_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                        if not line or line.startswith('#'):
                            continue

                        # è§£æURLå¹¶ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                        local_filename = self._url_to_filename(line)

                        # åªæ˜ å°„å®é™…å­˜åœ¨çš„æ–‡ä»¶
                        if local_filename in existing_files:
                            self.file_url_mapping[local_filename] = line

            self.logger.info(f"æ„å»ºäº† {len(self.file_url_mapping)} ä¸ªæ–‡ä»¶URLæ˜ å°„")
            for filename, url in sorted(self.file_url_mapping.items()):
                self.logger.debug(f"  {filename} -> {url}")

            return self.file_url_mapping

        except Exception as e:
            self.logger.error(f"æ„å»ºæ–‡ä»¶URLæ˜ å°„å¤±è´¥: {str(e)}")
            return {}

    def _url_to_filename(self, url: str) -> str:
        """
        å°†URLè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶å

        Args:
            url (str): åŸå§‹URL

        Returns:
            str: æœ¬åœ°æ–‡ä»¶å
        """
        try:
            parsed = urlparse(url)
            path = parsed.path.strip('/')

            if not path or path == "scaling-book":
                # ä¸»é¡µæƒ…å†µ
                return "scaling-book.html"

            # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºæ–‡ä»¶å
            if path.startswith("scaling-book/"):
                page_name = path.replace("scaling-book/", "")
                if page_name:
                    return f"{page_name}.html"

            # å¦‚æœè·¯å¾„ä¸åŒ…å«scaling-bookï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€æ®µ
            path_parts = path.split('/')
            if path_parts:
                return f"{path_parts[-1]}.html"

            return "scaling-book.html"

        except Exception as e:
            self.logger.warning(f"URLè½¬æ–‡ä»¶åå¤±è´¥ {url}: {str(e)}")
            return "unknown.html"

    def create_header_html(self, original_url: str) -> str:
        """
        ç”Ÿæˆé¡µå¤´ä¿¡æ¯çš„HTMLå†…å®¹

        Args:
            original_url (str): åŸå§‹æ–‡ç« URL

        Returns:
            str: ç”Ÿæˆçš„HTMLå­—ç¬¦ä¸²
        """
        header_html = f'''
        <div class="translation-info base-grid" style="margin-bottom: 20px;">
            <div style="grid-column: text;
                       display: flex;
                       align-items: center;
                       justify-content: space-between;
                       padding: 16px 0;
                       border-bottom: 1px solid var(--global-text-color-light, rgba(0,0,0,0.15));
                       font-size: 16px;
                       line-height: 1.5;
                       color: var(--global-text-color, currentColor);">
                <div style="display: flex;
                           flex-direction: column;
                           gap: 8px;">
                    <div>
                        <span style="font-weight: 600; color: var(--global-text-color, currentColor);">ğŸ”— è‹±æ–‡åŸæ–‡ï¼š</span>
                        <a href="{original_url}"
                           target="_blank"
                           rel="noopener noreferrer"
                           style="color: var(--global-theme-color, #004276);
                                  text-decoration: none;
                                  margin-left: 4px;"
                           onmouseover="this.style.textDecoration='underline'"
                           onmouseout="this.style.textDecoration='none'">
                           {original_url}
                        </a>
                    </div>
                    <div>
                        <span style="font-weight: 600; color: var(--global-text-color, currentColor);">âœï¸ ç¿»è¯‘ï¼š</span>
                        <span style="margin-left: 4px; color: var(--global-text-color, currentColor);">{self.translator_name}</span>
                    </div>
                </div>
                <div style="flex-shrink: 0;
                           display: flex;
                           flex-direction: column;
                           align-items: center;
                           gap: 6px;
                           margin-left: 20px;">
                    <img src="{self.wechat_qr_url}"
                         alt="å¾®ä¿¡äºŒç»´ç "
                         style="width: 80px;
                                height: 80px;
                                border-radius: 6px;
                                opacity: 0.9;"
                         loading="lazy">
                    <span style="font-size: 12px;
                                 color: var(--global-text-color-light, currentColor);
                                 opacity: 0.8;
                                 text-align: center;">
                        å¾®ä¿¡å…¬ä¼—å·
                    </span>
                </div>
            </div>
        </div>'''

        return header_html.strip()

    def find_insertion_point(self, soup: BeautifulSoup) -> Optional[Tag]:
        """
        æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®

        Args:
            soup (BeautifulSoup): è§£æåçš„HTMLæ–‡æ¡£

        Returns:
            Optional[Tag]: æ’å…¥ä½ç½®çš„æ ‡ç­¾ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
        """
        try:
            # æŸ¥æ‰¾ <div class="post distill"> æ ‡ç­¾
            post_distill = soup.find('div', class_='post distill')
            if post_distill:
                self.logger.debug("æ‰¾åˆ°æ’å…¥ç‚¹: <div class='post distill'>")
                return post_distill

            # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å« "post" å’Œ "distill" ç±»çš„div
            post_div = soup.find('div', class_=lambda x: x and 'post' in x and 'distill' in x)
            if post_div:
                self.logger.debug("æ‰¾åˆ°å¤‡é€‰æ’å…¥ç‚¹: div with 'post' and 'distill' classes")
                return post_div

            # æœ€åå¤‡é€‰ï¼šæŸ¥æ‰¾ d-title æ ‡ç­¾çš„çˆ¶å®¹å™¨
            d_title = soup.find('d-title')
            if d_title and d_title.parent:
                self.logger.debug("æ‰¾åˆ°å¤‡é€‰æ’å…¥ç‚¹: d-titleçš„çˆ¶å®¹å™¨")
                return d_title.parent

            self.logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„æ’å…¥ç‚¹")
            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾æ’å…¥ç‚¹å¤±è´¥: {str(e)}")
            return None

    def process_html_file(self, file_path: Path) -> bool:
        """
        å¤„ç†å•ä¸ªHTMLæ–‡ä»¶

        Args:
            file_path (Path): HTMLæ–‡ä»¶è·¯å¾„

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¤„ç†æ–‡ä»¶: {file_path.name}")

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„åŸå§‹URL
            filename = file_path.name
            if filename not in self.file_url_mapping:
                self.logger.warning(f"æœªæ‰¾åˆ°æ–‡ä»¶ {filename} çš„åŸå§‹URLæ˜ å°„ï¼Œè·³è¿‡å¤„ç†")
                self.stats['files_skipped'] += 1
                return True

            original_url = self.file_url_mapping[filename]

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()

            # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡é¡µå¤´ä¿¡æ¯
            if 'translation-info' in html_content:
                self.logger.info(f"â­ï¸  {filename}: å·²åŒ…å«é¡µå¤´ä¿¡æ¯ï¼Œè·³è¿‡å¤„ç†")
                self.stats['files_skipped'] += 1
                return True

            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')

            # æŸ¥æ‰¾æ’å…¥ç‚¹
            insertion_point = self.find_insertion_point(soup)
            if not insertion_point:
                self.logger.error(f"æœªæ‰¾åˆ° {filename} çš„æ’å…¥ç‚¹")
                self.stats['files_skipped'] += 1
                return False

            # ç”Ÿæˆé¡µå¤´ä¿¡æ¯HTML
            header_html = self.create_header_html(original_url)
            header_soup = BeautifulSoup(header_html, 'html.parser')

            # æ’å…¥é¡µå¤´ä¿¡æ¯ï¼ˆåœ¨å®¹å™¨çš„å¼€å¤´ï¼‰
            if insertion_point.contents:
                # åœ¨ç¬¬ä¸€ä¸ªå­å…ƒç´ ä¹‹å‰æ’å…¥
                insertion_point.insert(0, header_soup)
            else:
                # å¦‚æœå®¹å™¨ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ 
                insertion_point.append(header_soup)

            # ä¿å­˜ä¿®æ”¹åçš„HTML
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(str(soup))

            self.stats['files_processed'] += 1
            self.stats['files_modified'] += 1
            self.stats['headers_added'] += 1

            self.logger.info(f"âœ… {filename}: æˆåŠŸæ·»åŠ é¡µå¤´ä¿¡æ¯")
            return True

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return False

    def process_all_files(self) -> Dict[str, int]:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶

        Returns:
            Dict[str, int]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.info("å¼€å§‹æ‰¹é‡æ·»åŠ é¡µå¤´ä¿¡æ¯...")

            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.stats = {
                'files_processed': 0,
                'files_modified': 0,
                'headers_added': 0,
                'files_skipped': 0
            }

            # æ„å»ºæ–‡ä»¶URLæ˜ å°„
            if not self.build_file_url_mapping():
                self.logger.error("æ— æ³•æ„å»ºæ–‡ä»¶URLæ˜ å°„ï¼Œåœæ­¢å¤„ç†")
                return self.stats

            # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
            html_files = list(self.trans_dir.glob("*.html"))
            if not html_files:
                self.logger.warning("æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
                return self.stats

            self.logger.info(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            success_count = 0
            for html_file in html_files:
                if self.process_html_file(html_file):
                    success_count += 1

            # æ‰“å°ç»Ÿè®¡ç»“æœ
            self.logger.info("=" * 50)
            self.logger.info("æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
            self.logger.info(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {len(html_files)}")
            self.logger.info(f"  âœ… æˆåŠŸå¤„ç†: {success_count}")
            self.logger.info(f"  ğŸ“ ä¿®æ”¹æ–‡ä»¶: {self.stats['files_modified']}")
            self.logger.info(f"  ğŸ·ï¸  æ·»åŠ é¡µå¤´: {self.stats['headers_added']}")
            self.logger.info(f"  â­ï¸  è·³è¿‡æ–‡ä»¶: {self.stats['files_skipped']}")
            self.logger.info("=" * 50)

            return self.stats

        except Exception as e:
            self.logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            return self.stats

    def get_stats(self) -> Dict[str, int]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


# ä¾¿æ·å‡½æ•°
def add_headers_to_all_files(trans_dir: str = None) -> Dict[str, int]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºæ‰€æœ‰HTMLæ–‡ä»¶æ·»åŠ é¡µå¤´ä¿¡æ¯

    Args:
        trans_dir (str): ç¿»è¯‘æ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®

    Returns:
        Dict[str, int]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    adder = HeaderInfoAdder(trans_dir)
    return adder.process_all_files()


def add_header_to_single_file(file_path: str, trans_dir: str = None) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºå•ä¸ªæ–‡ä»¶æ·»åŠ é¡µå¤´ä¿¡æ¯

    Args:
        file_path (str): HTMLæ–‡ä»¶è·¯å¾„
        trans_dir (str): ç¿»è¯‘æ–‡ä»¶ç›®å½•

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    adder = HeaderInfoAdder(trans_dir)
    adder.build_file_url_mapping()
    return adder.process_html_file(Path(file_path))


# æµ‹è¯•å’Œä¸»å‡½æ•°
async def test_single_file():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶çš„é¡µå¤´ä¿¡æ¯æ·»åŠ åŠŸèƒ½"""
    print("=== æµ‹è¯•å•ä¸ªæ–‡ä»¶é¡µå¤´ä¿¡æ¯æ·»åŠ åŠŸèƒ½ ===")

    try:
        # åˆ›å»ºé¡µå¤´ä¿¡æ¯æ·»åŠ å™¨
        adder = HeaderInfoAdder()

        # æ„å»ºæ–‡ä»¶URLæ˜ å°„
        print("ğŸ”„ æ„å»ºæ–‡ä»¶URLæ˜ å°„...")
        file_mapping = adder.build_file_url_mapping()
        print(f"âœ… æ„å»ºäº† {len(file_mapping)} ä¸ªæ–‡ä»¶URLæ˜ å°„")

        # æ˜¾ç¤ºæ˜ å°„å†…å®¹
        print("\nğŸ“‹ æ–‡ä»¶URLæ˜ å°„è¡¨:")
        for filename, url in sorted(file_mapping.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  {filename} -> {url}")
        if len(file_mapping) > 5:
            print(f"  ... è¿˜æœ‰ {len(file_mapping) - 5} ä¸ªæ˜ å°„")

        # æµ‹è¯•å•ä¸ªæ–‡ä»¶ï¼šscaling-book.html
        test_file = Path(adder.trans_dir) / "scaling-book.html"

        if not test_file.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return

        print(f"\nğŸ”„ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # å¤„ç†æ–‡ä»¶
        success = adder.process_html_file(test_file)

        if success:
            print("âœ… å•ä¸ªæ–‡ä»¶æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {adder.get_stats()}")
        else:
            print("âŒ å•ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


async def process_all_pages():
    """å¤„ç†æ‰€æœ‰ç½‘é¡µçš„é¡µå¤´ä¿¡æ¯æ·»åŠ """
    print("=== æ‰¹é‡å¤„ç†æ‰€æœ‰ç½‘é¡µé¡µå¤´ä¿¡æ¯æ·»åŠ  ===")

    try:
        # åˆ›å»ºé¡µå¤´ä¿¡æ¯æ·»åŠ å™¨
        adder = HeaderInfoAdder()

        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        print("ğŸ”„ å¼€å§‹ä¸ºæ‰€æœ‰HTMLæ–‡ä»¶æ·»åŠ é¡µå¤´ä¿¡æ¯...")
        stats = adder.process_all_files()

        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰¹é‡é¡µå¤´ä¿¡æ¯æ·»åŠ å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°é‡: {stats['files_processed'] + stats['files_skipped']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {stats['files_processed']}")
        print(f"ğŸ“ ä¿®æ”¹æ–‡ä»¶æ•°: {stats['files_modified']}")
        print(f"ğŸ·ï¸  æ·»åŠ é¡µå¤´æ•°: {stats['headers_added']}")
        print(f"â­ï¸  è·³è¿‡æ–‡ä»¶æ•°: {stats['files_skipped']}")

        if stats['headers_added'] > 0:
            print(f"\nâœ¨ æˆåŠŸä¸º {stats['headers_added']} ä¸ªé¡µé¢æ·»åŠ äº†ç‰ˆæƒä¿¡æ¯å’Œç¿»è¯‘è€…ä¿¡æ¯ï¼")
            print("ğŸ“‹ ç°åœ¨æ‰€æœ‰é¡µé¢éƒ½åŒ…å«äº†åŸæ–‡é“¾æ¥å’Œç¿»è¯‘è€…ä¿¡æ¯ã€‚")
        else:
            print("\nâš ï¸  æ²¡æœ‰æ·»åŠ ä»»ä½•é¡µå¤´ä¿¡æ¯ï¼Œå¯èƒ½æ‰€æœ‰æ–‡ä»¶éƒ½å·²ç»å¤„ç†è¿‡äº†ã€‚")

        print("=" * 60)

        return stats

    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


async def main():
    """ä¸»å‡½æ•°"""
    from config.logging_config import setup_logging

    # è®¾ç½®æ—¥å¿—
    setup_logging('INFO')

    print("é¡µå¤´ä¿¡æ¯æ·»åŠ å·¥å…·")
    print("=" * 50)

    # å…ˆæµ‹è¯•å•ä¸ªæ–‡ä»¶
    await test_single_file()

    print("\n" + "=" * 50)

    # å†å¤„ç†æ‰€æœ‰ç½‘é¡µ
    await process_all_pages()

    print("\nå®Œæˆ!")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
