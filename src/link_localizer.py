"""
é“¾æ¥æœ¬åœ°åŒ–æ¨¡å—
å°†HTMLæ–‡ä»¶ä¸­çš„ç»å¯¹é“¾æ¥è½¬æ¢ä¸ºæœ¬åœ°ç›¸å¯¹é“¾æ¥

åˆ›å»ºæ—¶é—´ï¼š2024-12-19
é¡¹ç›®ï¼šç³»åˆ—æŠ€æœ¯æ–‡ç« ç¿»è¯‘
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

from config.settings import TRANS_DIR


class LinkLocalizer:
    """å°†HTMLæ–‡ä»¶ä¸­çš„ç»å¯¹é“¾æ¥è½¬æ¢ä¸ºæœ¬åœ°ç›¸å¯¹é“¾æ¥"""

    def __init__(self, trans_dir: str = None, urls_config: str = None):
        """
        åˆå§‹åŒ–é“¾æ¥æœ¬åœ°åŒ–å™¨

        Args:
            trans_dir (str): ç¿»è¯‘åæ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„TRANS_DIR
            urls_config (str): URLé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨config/urls.txt
        """
        self.logger = logging.getLogger(__name__)

        # è®¾ç½®ç›®å½•è·¯å¾„
        self.trans_dir = Path(trans_dir) if trans_dir else Path(TRANS_DIR)
        self.urls_config = urls_config or "src/config/urls.txt"

        # éªŒè¯ç›®å½•å­˜åœ¨
        if not self.trans_dir.exists():
            raise FileNotFoundError(f"ç¿»è¯‘ç›®å½•ä¸å­˜åœ¨: {self.trans_dir}")

        # URLæ˜ å°„å­—å…¸ï¼šç»å¯¹URL -> æœ¬åœ°æ–‡ä»¶å
        self.url_mapping: Dict[str, str] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'files_processed': 0,
            'files_modified': 0,
            'links_converted': 0,
            'links_skipped': 0
        }

        # åŸºç¡€åŸŸåæ¨¡å¼
        self.base_domain = "https://jax-ml.github.io/scaling-book"

        self.logger.info(f"é“¾æ¥æœ¬åœ°åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡ç›®å½•: {self.trans_dir}")

    def build_url_mapping(self) -> Dict[str, str]:
        """
        æ„å»ºURLåˆ°æœ¬åœ°æ–‡ä»¶çš„æ˜ å°„å…³ç³»

        Returns:
            Dict[str, str]: URLæ˜ å°„å­—å…¸
        """
        try:
            self.url_mapping.clear()

            # è·å–å®é™…å­˜åœ¨çš„HTMLæ–‡ä»¶
            existing_files = set()
            for html_file in self.trans_dir.glob("*.html"):
                existing_files.add(html_file.name)

            self.logger.info(f"æ‰¾åˆ° {len(existing_files)} ä¸ªç°æœ‰HTMLæ–‡ä»¶: {sorted(existing_files)}")

            # è¯»å–URLé…ç½®æ–‡ä»¶
            urls_file = Path(self.urls_config)
            if urls_file.exists():
                with open(urls_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                        if not line or line.startswith('#'):
                            continue

                        # è§£æURLå¹¶ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                        local_filename = self._url_to_filename(line)

                        # åªæ˜ å°„å®é™…å­˜åœ¨çš„æ–‡ä»¶
                        if local_filename in existing_files:
                            self.url_mapping[line] = local_filename
                            # åŒæ—¶å¤„ç†å¸¦å°¾éƒ¨æ–œæ çš„ç‰ˆæœ¬
                            if line.endswith('/'):
                                self.url_mapping[line.rstrip('/')] = local_filename
                            else:
                                self.url_mapping[line + '/'] = local_filename

            # å¤„ç†ä¸»é¡µç‰¹æ®Šæƒ…å†µ
            if "scaling-book.html" in existing_files:
                self.url_mapping[self.base_domain] = "scaling-book.html"
                self.url_mapping[self.base_domain + "/"] = "scaling-book.html"

            self.logger.info(f"æ„å»ºäº† {len(self.url_mapping)} ä¸ªURLæ˜ å°„")
            for url, filename in sorted(self.url_mapping.items()):
                self.logger.debug(f"  {url} -> {filename}")

            return self.url_mapping

        except Exception as e:
            self.logger.error(f"æ„å»ºURLæ˜ å°„å¤±è´¥: {str(e)}")
            return {}

    def _url_to_filename(self, url: str) -> str:
        """
        å°†URLè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶å

        Args:
            url (str): åŸå§‹URL

        Returns:
            str: æœ¬åœ°æ–‡ä»¶å
        """
        try:
            parsed = urlparse(url)
            path = parsed.path.strip('/')

            if not path or path == "scaling-book":
                # ä¸»é¡µæƒ…å†µ
                return "scaling-book.html"

            # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºæ–‡ä»¶å
            if path.startswith("scaling-book/"):
                page_name = path.replace("scaling-book/", "")
                if page_name:
                    return f"{page_name}.html"

            # å¦‚æœè·¯å¾„ä¸åŒ…å«scaling-bookï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€æ®µ
            path_parts = path.split('/')
            if path_parts:
                return f"{path_parts[-1]}.html"

            return "scaling-book.html"

        except Exception as e:
            self.logger.warning(f"URLè½¬æ–‡ä»¶åå¤±è´¥ {url}: {str(e)}")
            return "unknown.html"

    def _is_local_link(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦æœ¬åœ°åŒ–çš„é“¾æ¥

        Args:
            url (str): è¦æ£€æŸ¥çš„URL

        Returns:
            bool: æ˜¯å¦ä¸ºæœ¬åœ°é“¾æ¥
        """
        if not url:
            return False

        # è·³è¿‡é”šç‚¹é“¾æ¥
        if url.startswith('#'):
            return False

        # è·³è¿‡å…¶ä»–åè®®
        if url.startswith(('mailto:', 'tel:', 'javascript:')):
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡åŸŸåçš„é“¾æ¥
        return url.startswith(self.base_domain)

    def _convert_links_in_html(self, html_content: str) -> Tuple[str, int]:
        """
        è½¬æ¢HTMLå†…å®¹ä¸­çš„é“¾æ¥

        Args:
            html_content (str): åŸå§‹HTMLå†…å®¹

        Returns:
            Tuple[str, int]: (è½¬æ¢åçš„HTMLå†…å®¹, è½¬æ¢çš„é“¾æ¥æ•°é‡)
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            converted_count = 0

            # æŸ¥æ‰¾æ‰€æœ‰å¸¦hrefå±æ€§çš„æ ‡ç­¾
            for tag in soup.find_all(attrs={'href': True}):
                href = tag.get('href')

                if self._is_local_link(href):
                    # è§„èŒƒåŒ–URLï¼ˆç§»é™¤å°¾éƒ¨æ–œæ è¿›è¡ŒåŒ¹é…ï¼‰
                    normalized_url = href.rstrip('/')

                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶
                    if href in self.url_mapping:
                        local_file = self.url_mapping[href]
                        tag['href'] = local_file
                        converted_count += 1
                        self.logger.debug(f"è½¬æ¢é“¾æ¥: {href} -> {local_file}")
                    elif normalized_url in self.url_mapping:
                        local_file = self.url_mapping[normalized_url]
                        tag['href'] = local_file
                        converted_count += 1
                        self.logger.debug(f"è½¬æ¢é“¾æ¥: {href} -> {local_file}")
                    else:
                        self.logger.warning(f"æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶æ˜ å°„: {href}")
                        self.stats['links_skipped'] += 1

            # æŸ¥æ‰¾æ‰€æœ‰å¸¦srcå±æ€§çš„æ ‡ç­¾ï¼ˆå¤„ç†èµ„æºæ–‡ä»¶ï¼‰
            for tag in soup.find_all(attrs={'src': True}):
                src = tag.get('src')

                if self._is_local_link(src):
                    # å¯¹äºèµ„æºæ–‡ä»¶ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¸åŒçš„å¤„ç†ç­–ç•¥
                    # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºä¸»è¦å…³æ³¨é¡µé¢å¯¼èˆªé“¾æ¥
                    self.logger.debug(f"è·³è¿‡èµ„æºæ–‡ä»¶: {src}")

            return str(soup), converted_count

        except Exception as e:
            self.logger.error(f"è½¬æ¢HTMLé“¾æ¥å¤±è´¥: {str(e)}")
            return html_content, 0

    def process_html_file(self, file_path: Path) -> bool:
        """
        å¤„ç†å•ä¸ªHTMLæ–‡ä»¶

        Args:
            file_path (Path): HTMLæ–‡ä»¶è·¯å¾„

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¤„ç†æ–‡ä»¶: {file_path.name}")

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()

            # è½¬æ¢é“¾æ¥
            converted_content, converted_count = self._convert_links_in_html(original_content)

            # å¦‚æœæœ‰è½¬æ¢ï¼Œåˆ™ä¿å­˜æ–‡ä»¶
            if converted_count > 0:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(converted_content)

                self.stats['files_modified'] += 1
                self.stats['links_converted'] += converted_count
                self.logger.info(f"âœ… {file_path.name}: è½¬æ¢äº† {converted_count} ä¸ªé“¾æ¥")
            else:
                self.logger.info(f"â­ï¸  {file_path.name}: æ— éœ€è½¬æ¢")

            self.stats['files_processed'] += 1
            return True

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return False

    def process_all_files(self) -> Dict[str, int]:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶

        Returns:
            Dict[str, int]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.info("å¼€å§‹æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶...")

            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.stats = {
                'files_processed': 0,
                'files_modified': 0,
                'links_converted': 0,
                'links_skipped': 0
            }

            # æ„å»ºURLæ˜ å°„
            if not self.build_url_mapping():
                self.logger.error("æ— æ³•æ„å»ºUxRLæ˜ å°„ï¼Œåœæ­¢å¤„ç†")
                return self.stats

            # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
            html_files = list(self.trans_dir.glob("*.html"))
            if not html_files:
                self.logger.warning("æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
                return self.stats

            self.logger.info(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            success_count = 0
            for html_file in html_files:
                if self.process_html_file(html_file):
                    success_count += 1

            # æ‰“å°ç»Ÿè®¡ç»“æœ
            self.logger.info("=" * 50)
            self.logger.info("æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
            self.logger.info(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {len(html_files)}")
            self.logger.info(f"  âœ… æˆåŠŸå¤„ç†: {success_count}")
            self.logger.info(f"  ğŸ“ ä¿®æ”¹æ–‡ä»¶: {self.stats['files_modified']}")
            self.logger.info(f"  ğŸ”— è½¬æ¢é“¾æ¥: {self.stats['links_converted']}")
            self.logger.info(f"  â­ï¸  è·³è¿‡é“¾æ¥: {self.stats['links_skipped']}")
            self.logger.info("=" * 50)

            return self.stats

        except Exception as e:
            self.logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            return self.stats

    def get_stats(self) -> Dict[str, int]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


# ä¾¿æ·å‡½æ•°
def localize_all_links(trans_dir: str = None) -> Dict[str, int]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæœ¬åœ°åŒ–æ‰€æœ‰HTMLæ–‡ä»¶ä¸­çš„é“¾æ¥

    Args:
        trans_dir (str): ç¿»è¯‘æ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®

    Returns:
        Dict[str, int]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    localizer = LinkLocalizer(trans_dir)
    return localizer.process_all_files()


def localize_single_file(file_path: str, trans_dir: str = None) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šæœ¬åœ°åŒ–å•ä¸ªæ–‡ä»¶ä¸­çš„é“¾æ¥

    Args:
        file_path (str): HTMLæ–‡ä»¶è·¯å¾„
        trans_dir (str): ç¿»è¯‘æ–‡ä»¶ç›®å½•

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    localizer = LinkLocalizer(trans_dir)
    localizer.build_url_mapping()
    return localizer.process_html_file(Path(file_path))


# æµ‹è¯•å’Œä¸»å‡½æ•°
async def test_single_file():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶çš„é“¾æ¥æœ¬åœ°åŒ–åŠŸèƒ½"""
    print("=== æµ‹è¯•å•ä¸ªæ–‡ä»¶é“¾æ¥æœ¬åœ°åŒ–åŠŸèƒ½ ===")

    try:
        # åˆ›å»ºé“¾æ¥æœ¬åœ°åŒ–å™¨
        localizer = LinkLocalizer()

        # æ„å»ºURLæ˜ å°„
        print("ğŸ”„ æ„å»ºURLæ˜ å°„...")
        url_mapping = localizer.build_url_mapping()
        print(f"âœ… æ„å»ºäº† {len(url_mapping)} ä¸ªURLæ˜ å°„")

        # æ˜¾ç¤ºæ˜ å°„å†…å®¹
        print("\nğŸ“‹ URLæ˜ å°„è¡¨:")
        for url, filename in sorted(url_mapping.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {url} -> {filename}")
        if len(url_mapping) > 10:
            print(f"  ... è¿˜æœ‰ {len(url_mapping) - 10} ä¸ªæ˜ å°„")

        # æµ‹è¯•å•ä¸ªæ–‡ä»¶ï¼šscaling-book.html
        test_file = Path(localizer.trans_dir) / "scaling-book.html"

        if not test_file.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return

        print(f"\nğŸ”„ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶æ£€æŸ¥é“¾æ¥
        with open(test_file, 'r', encoding='utf-8') as f:
            original_content = f.read()

        # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦è½¬æ¢çš„é“¾æ¥
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(original_content, 'html.parser')

        print("\nğŸ” æŸ¥æ‰¾éœ€è¦è½¬æ¢çš„é“¾æ¥:")
        found_links = []
        for tag in soup.find_all(attrs={'href': True}):
            href = tag.get('href')
            if localizer._is_local_link(href):
                found_links.append(href)
                print(f"  å‘ç°é“¾æ¥: {href}")

        if not found_links:
            print("  âš ï¸  æœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„é“¾æ¥")
            return

        print(f"\nğŸ“Š æ‰¾åˆ° {len(found_links)} ä¸ªéœ€è¦è½¬æ¢çš„é“¾æ¥")

        # å¤„ç†æ–‡ä»¶
        success = localizer.process_html_file(test_file)

        if success:
            print("âœ… å•ä¸ªæ–‡ä»¶æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {localizer.get_stats()}")
        else:
            print("âŒ å•ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


async def process_all_pages():
    """å¤„ç†æ‰€æœ‰ç½‘é¡µçš„é“¾æ¥æœ¬åœ°åŒ–"""
    print("=== æ‰¹é‡å¤„ç†æ‰€æœ‰ç½‘é¡µé“¾æ¥æœ¬åœ°åŒ– ===")

    try:
        # åˆ›å»ºé“¾æ¥æœ¬åœ°åŒ–å™¨
        localizer = LinkLocalizer()

        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        print("ğŸ”„ å¼€å§‹å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶...")
        stats = localizer.process_all_files()

        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰¹é‡é“¾æ¥æœ¬åœ°åŒ–å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°é‡: {stats['files_processed']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {stats['files_processed']}")
        print(f"ğŸ“ ä¿®æ”¹æ–‡ä»¶æ•°: {stats['files_modified']}")
        print(f"ğŸ”— è½¬æ¢é“¾æ¥æ•°: {stats['links_converted']}")
        print(f"â­ï¸  è·³è¿‡é“¾æ¥æ•°: {stats['links_skipped']}")

        if stats['links_converted'] > 0:
            print(f"\nâœ¨ æˆåŠŸå°† {stats['links_converted']} ä¸ªç»å¯¹é“¾æ¥è½¬æ¢ä¸ºç›¸å¯¹é“¾æ¥ï¼")
            print("ğŸ“‹ ç°åœ¨æ‰€æœ‰é¡µé¢éƒ½å¯ä»¥ä½œä¸ºç‹¬ç«‹çš„æœ¬åœ°ç½‘ç«™è¿è¡Œäº†ã€‚")
        else:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦è½¬æ¢çš„é“¾æ¥ï¼Œå¯èƒ½æ‰€æœ‰é“¾æ¥éƒ½å·²ç»æ˜¯æœ¬åœ°åŒ–çš„äº†ã€‚")

        print("=" * 60)

        return stats

    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


async def main():
    """ä¸»å‡½æ•°"""
    from config.logging_config import setup_logging

    # è®¾ç½®æ—¥å¿—
    setup_logging('INFO')

    print("é“¾æ¥æœ¬åœ°åŒ–å·¥å…· - æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("=" * 50)

    # å¤„ç†æ‰€æœ‰ç½‘é¡µ
    await process_all_pages()

    print("\nå®Œæˆ!")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
